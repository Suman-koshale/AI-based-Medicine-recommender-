
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
from scipy.sparse import save_npz, load_npz
import os
import numpy as np

# --- Configuration ---
DATA_FILE = 'medicine_data.csv'

# Columns used to train the TF-IDF model (Combined Text Features)
FEATURE_COLUMNS = [
    'sub_category', 
    'product_name', 
    'salt_composition', 
    'product_manufactured', 
    'medicine_desc', 
    'side_effects', 
    'drug_interactions'
]

# CRITICAL IMPROVEMENT: FEATURE WEIGHTS
FEATURE_WEIGHTS = {
    'sub_category': 1, 
    'product_name': 1, 
    'salt_composition': 1, 
    'product_manufactured': 1, 
    'medicine_desc': 3,       # High Weight: 3x importance for primary usage/description
    'side_effects': 2,        # Medium Weight: 2x importance for symptoms/side effects
    'drug_interactions': 1
}

# Columns to be displayed in the final recommendation output
RECOMMENDATION_COLUMNS = ['product_name', 'salt_composition', 'product_manufactured']

# --- Output Artifacts ---
MODEL_FILE = 'tfidf_bigram_vectorizer.pkl'
MATRIX_FILE = 'tfidf_matrix.npz'
DATA_FILE_PKL = 'recommendation_data.pkl'
# ----------------------

def combine_weighted_features(row):
    """
    Combines text from feature columns, repeating the text based on its weight.
    """
    combined = []
    for col, weight in FEATURE_WEIGHTS.items():
        text = str(row[col]).lower().replace('\n', ' ') if pd.notna(row[col]) else ''
        combined.append(' '.join([text] * weight))
    return ' '.join(combined)


def train_and_save_recommendation_model():
    """
    Trains the TF-IDF Bigram model with feature weighting and saves all artifacts.
    """
    print(f"1. Loading dataset from: {DATA_FILE}")
    try:
        df = pd.read_csv(DATA_FILE)
    except FileNotFoundError:
        print(f"Error: File '{DATA_FILE}' not found. Place it in the current directory.")
        return False, None

    required_cols = list(FEATURE_WEIGHTS.keys()) + RECOMMENDATION_COLUMNS
    if not all(col in df.columns for col in required_cols):
        missing_cols = [col for col in required_cols if col not in df.columns]
        print(f"Error: Missing columns in CSV file: {missing_cols}")
        return False, None

    initial_count = len(df)
    
    # 2. Remove Duplicates
    print("2. Removing duplicates based on 'product_name' and 'salt_composition'...")
    df.drop_duplicates(subset=['product_name', 'salt_composition'], keep='first', inplace=True)
    df.reset_index(drop=True, inplace=True)
    
    final_count = len(df)
    
    print(f"Unique entries for training: {final_count}")

    # 3. Apply Feature Weighting
    print("3. Combining unique feature columns with WEIGHTS for training...")
    
    df['combined_features'] = df.apply(combine_weighted_features, axis=1)

    documents = df['combined_features']
    
    # 4. Initialize and Train the Vectorizer
    print("4. Initializing and training TF-IDF Vectorizer (Ngram Range 1-2)...")
    vectorizer = TfidfVectorizer(
        stop_words='english', 
        ngram_range=(1, 2)
    )
    
    vectorizer.fit(documents)
    
    print("5. Transforming unique documents into the TF-IDF Matrix...")
    tfidf_matrix = vectorizer.transform(documents)

    # 6. Save all necessary artifacts
    print("6. Saving model artifacts for deployment...")
    
    with open(MODEL_FILE, 'wb') as file:
        pickle.dump(vectorizer, file)
        
    save_npz(MATRIX_FILE, tfidf_matrix)

    recommendation_data = df[RECOMMENDATION_COLUMNS]
    with open(DATA_FILE_PKL, 'wb') as file:
        pickle.dump(recommendation_data, file)
    
    print("=========================================================")
    print("âœ… Training Complete! Artifacts Saved.")
    print("=========================================================")
    
    return True, df[RECOMMENDATION_COLUMNS]


def calculate_relevance_accuracy(top_k=5):
    """
    Performs the recommendation test and calculates 'Relevance Accuracy' 
    based on manual user input.
    """
    print("\n\n=========================================================")
    print(f"7. ACCURACY TEST: Top-{top_k} Recommendation Relevance Check")
    print("=========================================================")
    
    TEST_QUERY = 'severe ache in body and high fever'
    
    try:
        # Load artifacts
        with open(MODEL_FILE, 'rb') as file:
            vectorizer = pickle.load(file)
            
        tfidf_matrix = load_npz(MATRIX_FILE)
        
        with open(DATA_FILE_PKL, 'rb') as file:
            recommendation_data = pickle.load(file)
            
    except FileNotFoundError as e:
        print(f"Error loading required files for test: {e}. Ensure training was successful.")
        return
    
    # 1. Transform the user query and calculate Cosine Similarity
    query_vec = vectorizer.transform([TEST_QUERY])
    similarity_scores = cosine_similarity(query_vec, tfidf_matrix).flatten()
    top_indices = np.argsort(similarity_scores)[::-1][:top_k]
    
    # 2. Extract results
    top_scores = similarity_scores[top_indices]
    top_recommendations = recommendation_data.iloc[top_indices].reset_index(drop=True)
    
    # 3. Display Results
    print(f"TEST QUERY: '{TEST_QUERY}'")
    print(f"\n--- Top {top_k} Recommended Medicines ---")
    
    for i in range(len(top_recommendations)):
        score_percentage = top_scores[i] * 100
        
        product_name = top_recommendations.loc[i, 'product_name']
        salt_composition = top_recommendations.loc[i, 'salt_composition']
        manufacturer = top_recommendations.loc[i, 'product_manufactured']
        
        print(f"\n{i+1}. Product: {product_name}")
        print(f"   Similarity Score: {score_percentage:.2f}%")
        print(f"   Salt: {salt_composition}")
        print(f"   Manufacturer: {manufacturer}")

    # --- Manual Relevance Check and Accuracy Calculation ---
    print("\n---------------------------------------------------------")
    print("MANUAL ACCURACY CHECK:")
    
    # In a non-interactive environment, we cannot use input(), but we will simulate the check.
    
    # Non-interactive solution: Ask the user to mentally count and print a placeholder prompt.
    print(f"Please review the top {top_k} recommendations.")
    print("How many of these are RELEVANT (e.g., related to fever/pain relief)?")
    print("We will simulate the calculation with a placeholder result.")
    
    # *** PLACEHOLDER FOR RELEVANCE COUNT (You need to update this based on your run) ***
    relevant_count = 4 
    # *** END PLACEHOLDER ***
    
    accuracy_percent = (relevant_count / top_k) * 100
    
    print(f"\nRELEVANCE ACCURACY CALCULATION:")
    print(f"Relevant Results Found: {relevant_count} / {top_k}")
    print(f"Model Relevance Accuracy: {accuracy_percent:.2f}%")
    
    print("\n---------------------------------------------------------")
    print("TEST ENDED: The calculated percentage represents the model's relevance accuracy for this query.")
    print("---------------------------------------------------------")


if __name__ == "__main__":
    success, test_df = train_and_save_recommendation_model()
    if success:
        calculate_relevance_accuracy()

